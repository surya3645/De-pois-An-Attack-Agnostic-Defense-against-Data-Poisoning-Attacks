# De-pois-An-Attack-Agnostic-Defense-against-Data-Poisoning-Attacks

Project Team Members 
- Soundarya Soundarya 
- Surya Thatee
- Ashish shiwlani
- Yashwanth Praveen

# Introduction

Now a days everyone is fascinated with the self driving cars , but the company that builds these cars are facing many challenges in keeping there data safe ,that is used for model building. One of the main issue is data poisoning attacks,  where sophisticated attackers tamper with machine learning training data to produce undesirable outcomes, such vulnerabilities can cause serious risks to various security-critical domains such as self-driving cars, biometric identity recognition and computer vision. For Example, In self driving Cars, the attackers try to disrupt the Machine Learning model by adding malicious information in the training data . As a result of this the ML model reads stop sign as speed limit, then these type of misleading actions can create serious threat to a life.

# Tools

- Run the code on the Google Collab . So you do not need to install any type of application . Add all the files in the google drive , then login Google Colab using the gmail account and access your saved files form the drive by providing the file path as an input .

# Dataset

MNIST Datasbase - handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.

Train and test datasets are found in the following link
http://yann.lecun.com/exdb/mnist/



# Modelling

De-Pois Design Process involves three stages 

Start by executing the generator_CGAN_authen.py to generate sufficient synthetic training data with a similar distribution of Clean Dataset. Then the mimic_model_construction.py whose aim is to imitate the target model and using the constructed mimic model we distinguish the poisoined samples from the clean ones . Lastly in Poisoined Data Recognition we employ a detection boundary to set apart the poisoned samples from clean ones. If the mimic modelâ€™s output is lower than our detection boundary, the sample is then regarded as being poisoned.

Poisoned Data is generated by using either of the mnist_direct.py and mnist_generative.py. 


# References

- De-pois Research paper :https://arxiv.org/pdf/2105.03592.pdf
- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks : https://arxiv.org/abs/1511.06434
- MNIST Database : http://yann.lecun.com/exdb/mnist/
- https://secml.readthedocs.io/en/stable/tutorials/07-NeuralNetworks-MNIST.html
- For generating poisoning data using GP-attack: https://github.com/yangcf10/Poisoning-attack
